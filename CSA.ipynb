{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bff9076b-4cf4-4c72-9835-d08eb06f49c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_2084\\2897160439.py:12: DtypeWarning: Columns (1,11,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset preview:\n",
      "   Store        Date  Temperature  Fuel_Price  MarkDown1  MarkDown2  \\\n",
      "0    1.0  05-02-2010        42.31       2.572        NaN        NaN   \n",
      "1    1.0  12-02-2010        38.51       2.548        NaN        NaN   \n",
      "2    1.0  19-02-2010        39.93       2.514        NaN        NaN   \n",
      "3    1.0  26-02-2010        46.63       2.561        NaN        NaN   \n",
      "4    1.0  05-03-2010        46.50       2.625        NaN        NaN   \n",
      "\n",
      "   MarkDown3  MarkDown4  MarkDown5         CPI  Unemployment IsHoliday  Dept  \\\n",
      "0        NaN        NaN        NaN  211.096358         8.106     False     1   \n",
      "1        NaN        NaN        NaN  211.242170         8.106      True     1   \n",
      "2        NaN        NaN        NaN  211.289143         8.106     False     1   \n",
      "3        NaN        NaN        NaN  211.319643         8.106     False     1   \n",
      "4        NaN        NaN        NaN  211.350143         8.106     False     1   \n",
      "\n",
      "   Weekly_Sales  IsHoliday.1 Type      Size  \n",
      "0      24924.50        False    A  151315.0  \n",
      "1      46039.49         True    A  202307.0  \n",
      "2      41595.55        False    B   37392.0  \n",
      "3      19403.54        False    A  205863.0  \n",
      "4      21827.90        False    B   34875.0  \n",
      "\n",
      "Column Names in the Dataset:\n",
      "Index(['Store', 'Date', 'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2',\n",
      "       'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment',\n",
      "       'IsHoliday', 'Dept', 'Weekly_Sales', 'IsHoliday.1', 'Type', 'Size'],\n",
      "      dtype='object')\n",
      "\n",
      "Data Summary:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 421570 entries, 0 to 421569\n",
      "Data columns (total 17 columns):\n",
      " #   Column        Non-Null Count   Dtype  \n",
      "---  ------        --------------   -----  \n",
      " 0   Store         8190 non-null    float64\n",
      " 1   Date          8190 non-null    object \n",
      " 2   Temperature   8190 non-null    float64\n",
      " 3   Fuel_Price    8190 non-null    float64\n",
      " 4   MarkDown1     4032 non-null    float64\n",
      " 5   MarkDown2     2921 non-null    float64\n",
      " 6   MarkDown3     3613 non-null    float64\n",
      " 7   MarkDown4     3464 non-null    float64\n",
      " 8   MarkDown5     4050 non-null    float64\n",
      " 9   CPI           7605 non-null    float64\n",
      " 10  Unemployment  7605 non-null    float64\n",
      " 11  IsHoliday     8190 non-null    object \n",
      " 12  Dept          421570 non-null  int64  \n",
      " 13  Weekly_Sales  421570 non-null  float64\n",
      " 14  IsHoliday.1   421570 non-null  bool   \n",
      " 15  Type          45 non-null      object \n",
      " 16  Size          45 non-null      float64\n",
      "dtypes: bool(1), float64(12), int64(1), object(3)\n",
      "memory usage: 51.9+ MB\n",
      "None\n",
      "\n",
      "Missing Values:\n",
      "Store           413380\n",
      "Date            413380\n",
      "Temperature     413380\n",
      "Fuel_Price      413380\n",
      "MarkDown1       417538\n",
      "MarkDown2       418649\n",
      "MarkDown3       417957\n",
      "MarkDown4       418106\n",
      "MarkDown5       417520\n",
      "CPI             413965\n",
      "Unemployment    413965\n",
      "IsHoliday       413380\n",
      "Dept                 0\n",
      "Weekly_Sales         0\n",
      "IsHoliday.1          0\n",
      "Type            421525\n",
      "Size            421525\n",
      "dtype: int64\n",
      "\n",
      "Cleaned Data Sample:\n",
      "Empty DataFrame\n",
      "Columns: [Store, Date, Temperature, Fuel_Price, MarkDown1, MarkDown2, MarkDown3, MarkDown4, MarkDown5, CPI, Unemployment, IsHoliday, Dept, Weekly_Sales, IsHoliday.1, Type, Size]\n",
      "Index: []\n",
      "Error: The following features are not in the dataset: ['TotalSpent', 'TotalPurchases']\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'C:/Users/DELL/OneDrive/Documents/retail data set.csv'  # Update the path if necessary\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(\"Dataset preview:\")\n",
    "print(df.head())\n",
    "\n",
    "# Display all columns to identify potential features for clustering\n",
    "print(\"\\nColumn Names in the Dataset:\")\n",
    "print(df.columns)\n",
    "\n",
    "# Basic data information and missing values check\n",
    "print(\"\\nData Summary:\")\n",
    "print(df.info())\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Step 1: Data Preprocessing\n",
    "# Dropping rows with missing values for simplicity; you may use imputation techniques if necessary\n",
    "df_cleaned = df.dropna()\n",
    "\n",
    "# Display a sample of the cleaned data\n",
    "print(\"\\nCleaned Data Sample:\")\n",
    "print(df_cleaned.head())\n",
    "\n",
    "# Step 2: Feature Selection\n",
    "# Modify the feature selection based on your dataset's actual column names\n",
    "# Let's assume we found relevant columns like 'TotalSpent' and 'TotalPurchases' in the previous output.\n",
    "# Change the feature names accordingly to your dataset's actual columns.\n",
    "\n",
    "# Example: Adjust this based on your actual column names\n",
    "# Replace 'TotalSpent' and 'TotalPurchases' with appropriate column names from your dataset\n",
    "features = ['TotalSpent', 'TotalPurchases']  # Modify based on your dataset\n",
    "\n",
    "# Check if the specified features exist in the dataset\n",
    "missing_features = [feature for feature in features if feature not in df_cleaned.columns]\n",
    "\n",
    "if missing_features:\n",
    "    print(f\"Error: The following features are not in the dataset: {missing_features}\")\n",
    "else:\n",
    "    # Step 3: Selecting Features for Clustering\n",
    "    X = df_cleaned[features]\n",
    "    \n",
    "    # Step 4: Feature Scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Step 5: K-means Clustering\n",
    "    # Define a function to determine the optimal number of clusters using the Elbow Method\n",
    "    def optimal_kmeans(X_scaled):\n",
    "        wcss = []\n",
    "        for i in range(1, 11):\n",
    "            kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)\n",
    "            kmeans.fit(X_scaled)\n",
    "            wcss.append(kmeans.inertia_)\n",
    "        plt.plot(range(1, 11), wcss)\n",
    "        plt.title('Elbow Method to Determine Optimal Number of Clusters')\n",
    "        plt.xlabel('Number of clusters')\n",
    "        plt.ylabel('WCSS')\n",
    "        plt.show()\n",
    "\n",
    "    # Plot the Elbow Curve to determine the optimal number of clusters\n",
    "    optimal_kmeans(X_scaled)\n",
    "\n",
    "    # Fit the K-means model with the optimal number of clusters\n",
    "    optimal_clusters = 3  # Set the number of clusters based on the elbow method (modify based on your data)\n",
    "    kmeans = KMeans(n_clusters=optimal_clusters, init='k-means++', random_state=42)\n",
    "    kmeans_labels = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "    # Step 6: Analyze K-means Cluster Characteristics\n",
    "    df_cleaned['KMeans_Cluster'] = kmeans_labels\n",
    "    print(\"\\nK-Means Cluster Counts:\")\n",
    "    print(df_cleaned['KMeans_Cluster'].value_counts())\n",
    "\n",
    "    # Step 7: DBSCAN Clustering\n",
    "    # Choose eps and min_samples based on your data, adjust accordingly\n",
    "    dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "    dbscan_labels = dbscan.fit_predict(X_scaled)\n",
    "\n",
    "    # Step 8: Analyze DBSCAN Cluster Characteristics\n",
    "    df_cleaned['DBSCAN_Cluster'] = dbscan_labels\n",
    "    print(\"\\nDBSCAN Cluster Counts:\")\n",
    "    print(df_cleaned['DBSCAN_Cluster'].value_counts())\n",
    "\n",
    "    # Step 9: Evaluate Clustering Results\n",
    "    # Silhouette Score for K-means\n",
    "    kmeans_silhouette = silhouette_score(X_scaled, kmeans_labels)\n",
    "    print(f\"K-means Silhouette Score: {kmeans_silhouette}\")\n",
    "\n",
    "    # Silhouette Score for DBSCAN\n",
    "    dbscan_silhouette = silhouette_score(X_scaled, dbscan_labels) if len(set(dbscan_labels)) > 1 else -1\n",
    "    print(f\"DBSCAN Silhouette Score: {dbscan_silhouette}\")\n",
    "\n",
    "    # Step 10: Visualize the Clusters (K-means)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(x=X_scaled[:, 0], y=X_scaled[:, 1], hue=kmeans_labels, palette='Set1', s=100)\n",
    "    plt.title('Customer Segments (K-means Clustering)')\n",
    "    plt.xlabel('Feature 1: TotalSpent')\n",
    "    plt.ylabel('Feature 2: TotalPurchases')\n",
    "    plt.show()\n",
    "\n",
    "    # Step 11: Visualize the Clusters (DBSCAN)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(x=X_scaled[:, 0], y=X_scaled[:, 1], hue=dbscan_labels, palette='Set2', s=100)\n",
    "    plt.title('Customer Segments (DBSCAN Clustering)')\n",
    "    plt.xlabel('Feature 1: TotalSpent')\n",
    "    plt.ylabel('Feature 2: TotalPurchases')\n",
    "    plt.show()\n",
    "\n",
    "    # Optional: Save the results to a new CSV\n",
    "    df_cleaned.to_csv('C:/Users/DELL/OneDrive/Documents/customer_segmentation_output.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ad7ef5-d284-4b70-9782-be9ae79c503a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
